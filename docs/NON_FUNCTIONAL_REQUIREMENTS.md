# Non-Functional Requirements

| Area               | Target / Constraint                                                   | Source        | Design Element(s)                                                                  | Validation Method(s)                                                                 |
|--------------------|------------------------------------------------------------------------|----------------|---------------------------------------|----------------------------------------------------------------------------------------|
| **Availability**   | ≥ 99.9% uptime. Platform must survive AZ loss or pod crashes          | Brief         | Multi-AZ Kafka, Redis, API pods, OpenSearch; Pilot-light DR region                 | Chaos testing (AZ-level and pod-level failures). Verify ALB target health removal within 5 s |
| **Latency (p95)**  | ≤ 40 ms end-to-end API search response                                 | Brief         | Fast index-query, ID-based Redis price lookup                                      | Load tests, profiling (latency hotspots), RED metrics (Rate/Errors/Duration)           |
| **Throughput**     | Must support high sustained read and ingest rates (e.g. 100+ QPS)      | Design Target | Kafka fan-out, Flink autoscaling, tuned Kafka partitioning                         | Soak tests (long-duration load), Kafka lag, Flink backpressure metrics                 |
| **Retention**      | Raw events must be archived for ≥ 1 year                               | Brief         | Kafka → S3 (with lifecycle policy)                    | Confirm S3 lifecycle config                 |
| **Replayability**  | Minimum 3-day replay window for pipeline recovery and data reconciliation                      | Design Target | Kafka topic retention (3–7 days), Flink replay job support                         | Run manual replay dry-run, confirm Flink job rehydrates from Kafka/savepoint           |
| **Index Freshness**| ≤ 1 s delay between ingestion and searchability                        | Design Target | OpenSearch `refresh_interval` controls how often newly indexed documents become visible to search. Setting it to 1s ensures low-latency searchability without excessive overhead                                                 | Track p95 ingestion-to-query delta                                                     |
| **Durability**     | No data loss on AZ or node failure                                     | Brief         | Flink checkpoints to RocksDB, Kafka replication factor = 3 (each partition stored in 3 brokers), DynamoDB multi-AZ                             | Simulated AZ loss, checkpoint restore, data consistency checks                         |
| **DR Recovery**    | RPO < 5 s, RTO < 15 min                                                | Design Target | MirrorMaker 2 (replicates Kafka topics from one cluster to another), pilot-light replica stack, IaC-based region activation              | Fire-drill simulation, validate data freshness post-recovery                           |
| **Maintainability**| System must support onboarding new supplier feeds quickly              | Brief         | Adapter abstraction per supplier, connector isolation (reduces blast radius from bad feeds), schema registry (enforces structural consistency)             | Time-to-integrate new feed; error rate during first sync                               |
| **Security**       | Must comply with fare display rules and data protection expectations   | Brief         | TLS in transit, IAM-scoped access, audit logs for sensitive flows, all data encrypted in transit and at rest                  | Config reviews, automated scans, audit trail, automated compliance scanning (e.g. AWS Config)                      |
| **Data Quality**   | Must detect and reconcile pricing anomalies and conflicting flight information | Brief         | Flink deduplication with source ranking (i.e. prioritises high-trust providers during merge to resolve conflicting data deterministically); merge strategy with windowed state and outlier detection (e.g. sudden price drops) | Simulated conflict injection, anomaly detection metrics, alert thresholds |
| **Search Flexibility and Segmentation¹** | Must support tailored results by brand, locale, and customer segment | Brief | Search-time filtering and boosting using OpenSearch field tags (e.g. brand, locale, loyalty-partner); user segment inferred from token or request context | Integration tests with personalised search scenarios; audit query templates |
| **Dynamic Configuration Support** | Must support pricing variants by brand/segment with minimal code change | Brief | Config-driven fare modifiers; dynamic targeting via locale, loyalty, or segment tags | Variant tracking dashboards, feature flag logs, rollout monitoring |

¹ For future hyper-personalisation, the platform could integrate OpenSearch with OpenAI using Retrieval-Augmented Generation (RAG), enabling dynamic content generation based on user context and preferences. This would support advanced features like personalised fare suggestions, loyalty programme prompts, and tailored travel packages.