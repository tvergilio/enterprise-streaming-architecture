# Non-Functional Requirements

| Area               | Target / Constraint                                                   | Source        | Design Element(s)                                                                  | Validation Method(s)                                                                 |
|--------------------|------------------------------------------------------------------------|----------------|---------------------------------------|----------------------------------------------------------------------------------------|
| **Availability**   | ≥ 99.9% uptime. Platform must survive AZ loss or pod crashes          | Brief         | Multi-AZ Kafka, Redis, API pods, OpenSearch; Pilot-light DR region                 | Chaos testing (AZ-level and pod-level failures). Verify ALB target health removal within 5 s |
| **Latency (p95)**  | ≤ 40 ms end-to-end API search response (brief says "acceptable response times")                                 | Design Target         | Fast index-query, ID-based Redis price lookup                                      | Load tests, profiling (latency hotspots), RED metrics (Rate/Errors/Duration)           |
| **Ingest Throughput (Data)**     | Pipeline must process high-volume batch files and continuous streams from 15+ providers without backpressure      | Brief | Kafka fan-out, Flink autoscaling, tuned Kafka partitioning to maximise parallelism                         | Soak tests for simulated data feeds (long-duration load); monitor Kafka consumer lag and Flink backpressure metrics                 |
| **Read Throughput (Search)** | API must handle ~3,000 QPS at peak traffic| Brief | Horizontally-scaled API pods; optimised OpenSearch queries; multi-level caching                                | Load tests simulating 5:1 peak-to-average traffic ratio; monitor p99 latency and error rates under load                                     |
| **Retention**      | Raw events must be archived for ≥ 1 year                               | Brief         | Kafka → S3 (with lifecycle policy)                    | Confirm S3 lifecycle config                 |
| **Replayability**  | Minimum 3-day replay window for pipeline recovery and reconciliation of late-arriving or out-of-order events | Design Target | Kafka topic retention (3–7 days), Flink replay job support                         | Run manual replay dry-run, confirm Flink job rehydrates from Kafka/savepoint           |
| **Index Freshness**| ≤ 1 s delay between ingestion and searchability                        | Design Target | OpenSearch `refresh_interval` controls how often newly indexed documents become visible to search. Setting it to 1s ensures low-latency searchability without excessive overhead                                                 | Track p95 ingestion-to-query delta                                                     |
| **Durability**     | No data loss on AZ or node failure                                     | Brief         | Flink checkpoints to RocksDB, Kafka replication factor = 3 (each partition stored in 3 brokers), DynamoDB multi-AZ                             | Simulated AZ loss, checkpoint restore, data consistency checks                         |
| **DR Recovery**    | RPO < 5 s, RTO < 15 min                                                | Design Target | MirrorMaker 2 (replicates Kafka topics from one cluster to another), pilot-light replica stack, IaC-based region activation              | Fire-drill simulation, validate data freshness post-recovery                           |
| **Maintainability**| System must support onboarding new supplier feeds quickly              | Brief         | Adapter abstraction per supplier, connector isolation (reduces blast radius from bad feeds), schema registry (enforces structural consistency)             | Time-to-integrate new feed; error rate during first sync                               |
| **Security**       | Must comply with fare display rules and data protection expectations   | Brief         | TLS in transit, IAM-scoped access, audit logs for sensitive flows, all data encrypted in transit and at rest, PII awareness in logs and events  | Config reviews, automated scans, audit trail, automated compliance scanning (e.g. AWS Config)                      |
| **Data Quality**   | Must detect and reconcile pricing anomalies and conflicting flight information | Brief         | Flink deduplication with source ranking (i.e. prioritises high-trust providers during merge to resolve conflicting data deterministically); merge strategy with windowed state and outlier detection (e.g. sudden price drops) | Simulated conflict injection, anomaly detection metrics, alert thresholds |
| **Search Flexibility and Segmentation¹** | Must support tailored results by brand, locale, and customer segment | Brief | Search-time filtering and boosting using OpenSearch field tags (e.g. brand, locale, loyalty-partner); user segment inferred from token or request context | Integration tests with personalised search scenarios; audit query templates |
| **Dynamic Configuration Support** | Must support pricing variants by brand/segment with minimal code change | Brief | Config-driven fare modifiers; dynamic targeting via locale, loyalty, or segment tags | Variant tracking dashboards, feature flag logs, rollout monitoring |
| **Cost Efficiency**| Minimise cloud spend by dynamically scaling resources and leveraging optimal pricing models | Brief | Autoscaling on all compute layers (API, Flink); use of spot/preemptible instances for tolerant workloads; ARM-based compute (e.g., AWS Graviton); S3 Lifecycle Policies for archival storage | Track key metrics (e.g., cost-per-search); regular cost analysis via cloud provider tools; automated budget alerts |

¹ For future hyper-personalisation, the platform could integrate OpenSearch with OpenAI using Retrieval-Augmented Generation (RAG), enabling dynamic content generation based on user context and preferences. This would support advanced features like personalised fare suggestions, loyalty programme prompts, and tailored travel packages. While not required in the current scope, this platform allows evolution toward hyper-personalisation using RAG.